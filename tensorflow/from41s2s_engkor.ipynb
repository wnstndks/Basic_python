{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMFT/sBAyD1UNrv0/trdJwX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"znzQfAlQ-opV"},"outputs":[],"source":["# seq2seq를 이용해 글자 단위 번역기 (영어 -> 한국어)\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","import numpy as np\n","\n","batch_size = 64\n","epochs = 1000\n","latent_dim = 1024 # 인코딩 공간의 잠재 차원\n","data_path ='kor.txt'\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","lines = open(data_path, encoding='utf8').read().split('\\n')\n","print(len(lines))"]},{"cell_type":"code","source":["# for line in lines[:len(lines) - 1]:\n","for line in lines[:2000]:\n","  input_text, target_text, _ = line.split('\\t')\n","  target_text = '\\t' + target_text + '\\n' # tab을 문장의 시작(SOS), enter를 문장의 끝(EOS)\n","  input_texts.append(input_text)\n","  target_texts.append(target_text)\n","  for char in input_text:\n","    if char not in input_characters:\n","      input_characters.add(char) # 중복을 배제한 영어 글자 기억\n","\n","  for char in target_text:\n","    if char not in target_characters:\n","      target_characters.add(char) # 중복을 배제한 한국어 글자 기억\n","\n","print(input_texts)\n","print(target_texts)\n","print(input_characters)\n","print(target_characters)\n","\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","\n","print('특수문자 포함 영어 관련 글자 수 :', num_encoder_tokens)\n","print('특수문자 포함 영어 관련 글자 :', input_characters)\n","print('특수문자 포함 한국어 관련 글자 수 :', num_decoder_tokens)\n","print('특수문자 포함 한국어 관련 글자 :', target_characters)\n","\n","max_encoder_seq_len = max([len(t) for t in input_texts])\n","max_decoder_seq_len = max([len(t) for t in target_texts])\n","print('영어 단어 중 가장 긴 단어 글자 수 :', max_encoder_seq_len)\n","print('한국어 단어 중 가장 긴 단어 글자 수 :', max_decoder_seq_len)\n","\n","# 글자 집합에 글자 단위로 저장된 각 글자에 대해 index를 부여\n","input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n","print(input_token_index) # 영어\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n","print(target_token_index) # 한국어\n","\n","encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_len, num_encoder_tokens), dtype='float32')\n","print(encoder_input_data.shape)\n","decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_len, num_decoder_tokens), dtype='float32')\n","print(decoder_input_data.shape)\n","decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_len, num_decoder_tokens), dtype='float32')\n","print(decoder_target_data.shape)\n","\n","# 0으로 채워진 배열에 해당 글자가 있는 지점에는 1을 기억\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","  # print(i, input_text, target_text)\n","  for t, char in enumerate(input_text):\n","    encoder_input_data[i, t, input_token_index[char]] = 1. # 해당 글자가 있는 경우 1을 기억\n","  for t, char in enumerate(target_text):\n","    decoder_input_data[i, t, target_token_index[char]] = 1. # 해당 글자가 있는 경우 1을 기억\n","    if t > 0:\n","      decoder_target_data[i, t-1, target_token_index[char]] = 1. # decoder_target_data가 한 step 앞서 진행됨. 시작 문자는 포함되지 않음.\n","  # print(encoder_input_data[[5]])"],"metadata":{"id":"yii6wFKoO-ED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 입력 시퀀스를 정의 후 처리 : Functional api 사용\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder = LSTM(latent_dim, return_state = True) # return_sequences = True/False에 상관없이 마지막 은닉상태를 출력함\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)  # state_h:은닉 상태, state_c:셀 상태\n","\n","# encoder_outputs은 버리고 state_h, state_c만 유지 : context vector\n","encoder_states = [state_h, state_c]  # 이게 바로 context vector(문맥 벡터)!!\n","\n","# 디코더 설계\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)  # 모든 시점의 은닉상태와 마지막 시점의 은닉상태를 출력\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","# 인코더_입력_데이터 및 디코더_입력_데이터를 디코더_타겟_데이터로 변환하는 모델을 정의!!\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","print(model.summary())\n","\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","model.fit([encoder_input_data,decoder_input_data],decoder_target_data,batch_size=batch_size,\n","          epochs=epochs, validation_split=0.2,verbose=2)\n","\n","model.save('tf42.hdf5')\n","\n"],"metadata":{"id":"MgqVCvkfAqDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 번역 동작 단계\n","# 1) 번역하고자 하는 입력문장이 인코더에 들어와서 은닉상태와 셀상태를 얻는다.\n","# 2) 상태와 <sos>에 해당하는 '\\t'를 디코더로 보낸다.\n","# 3) 디코더가 <eos>에 해당하는 '\\n'이 나올때까지 다음 문자를 예측하는 행동을 반복한다.\n","\n","# seq2seq 번역기 동작시키기\n","encoder_model = Model(encoder_inputs, encoder_states) # 인코더 정의\n","# 디코더 설계\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n","# 문장의 다음 단어를 예측하기 위해 초기상태를 이전 시점의 상태로 사용한다.\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n","decoder_states = [state_h, state_c] # 훈련상태와 달리 LSTM이 반환하는 은닉상태와 셀상태를 가진 state_h, state_c를 기억\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model([decoder_inputs]+decoder_state_inputs,[decoder_outputs]+decoder_states)\n","\n","# 토큰 인덱스를 역방향으로 조화하여 시퀀스를 디코딩하기. 글자로 부터 인덱스를 얻는 것이 아니라 인덱스에서 글자 얻기\n","reverse_input_char_index = dict((i,char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict((i,char) for char, i in target_token_index.items())\n","print(reverse_input_char_index)\n","print(reverse_target_char_index)"],"metadata":{"id":"XhC256ROvE_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def decode_sequence(input_seq):\n","  # 입력으로부터 인코더의 상태를 얻음\n","  states_value = encoder_model.predict(input_seq,verbose=0)\n","\n","  # <SOS>에 해당하는 원-핫 벡터 생성\n","  target_seq = np.zeros((1, 1, num_decoder_tokens))\n","  target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","  stop_condition = False\n","  decoded_sentence = \"\"\n","\n","  # stop_condition이 True가 될 때까지 루프 반복\n","  while not stop_condition:\n","    # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n","    output_tokens, h, c = decoder_model.predict([target_seq] + states_value,verbose=0)\n","\n","    # 예측 결과를 문자로 변환\n","    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","    sampled_char = reverse_target_char_index[sampled_token_index]\n","\n","    # 현재 시점의 예측 문자를 예측 문장에 추가\n","    decoded_sentence += sampled_char\n","\n","    # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n","    if (sampled_char == '\\n' or\n","        len(decoded_sentence) > max_decoder_seq_len):\n","        stop_condition = True\n","\n","    # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    target_seq[0, 0, sampled_token_index] = 1.\n","\n","    # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n","    states_value = [h, c]\n","\n","  return decoded_sentence\n","\n","for seq_index in [3,10,1000]: # 입력 문장의 인덱스\n","  input_seq = encoder_input_data[seq_index:seq_index+1]\n","  decoded_sentence = decode_sequence(input_seq)\n","  print(35 * \"-\")\n","  print('입력 문장:', input_texts[seq_index])\n","  print('번역 문장:', decoded_sentence) # '\\n'을 빼고 출력"],"metadata":{"id":"yBfAyYhS6Pwg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"D10wa_vV8Dra"},"execution_count":null,"outputs":[]}]}
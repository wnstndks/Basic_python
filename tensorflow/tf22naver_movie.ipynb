{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMXMM11svTvqyLTwPvsXccB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 네이버 영화 리뷰 데이터를 이용해 단어 간 유사도 확인"],"metadata":{"id":"yWD3Grrq4Wkd"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"id":"5qchS_RE45bS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DEYPCms4HIx"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from konlpy.tag import Okt\n","from gensim.models.word2vec import Word2Vec\n","import urllib.request"]},{"cell_type":"code","source":["urllib.request.urlretrieve('https://raw.githubusercontent.com/pykwon/python/master/testdata_utf8/ratings_train.txt', filename='ratings.txt') # git raw 파일에서 다운로드하기\n","train_data= pd.read_table('ratings.txt')\n","print(train_data[:5],'\\n\\n',len(train_data)) # 150000\n","print(train_data.isnull().values.any())\n","print(train_data.info())\n","\n","train_data=train_data.dropna(how='any')\n","print(len(train_data)) #149995\n","\n","train_data['document'] = train_data['document'].str.replace('[^가-힣 ]',\"\") # 한글과 공백을 제외한 나머지 문자들을 다 지움\n","print(train_data[:5])\n","\n","# 불용어(stop words) : 문장에서 의미가 없는( - 분석에 영향을 주지 않는) 단어 token => 없애주는 것이 좋다.\n","# 영문의 경우, nltk에 이미 불용어 사전으로 등록이 되어있으나 한글은 매우 동적이므로 불용어 사전이 없다.\n","stopwords=['의','가','는','은','잘','을','를','으로','하여','에','와','한','하다','앗','아','그래서'] #내가 생각하기에 불용어인거 넣어준것\n","okt=Okt()\n","tokenized_data = []\n","for sen in train_data['document']:\n","    temp = okt.morphs(sen, stem=True) #어근 처리\n","    temp=[word for word in temp if not word in stopwords] #불용어를 제거하고 토큰 처리를 해주어야 한다,\n","    tokenized_data.append(temp)\n"],"metadata":{"id":"T2qvk-Xq4x6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('글 최대 길이 : ', max(len(i) for i in tokenized_data))\n","print('글 평균 길이 : ', sum(map(len,tokenized_data))/len(tokenized_data))\n","\n","plt.hist([len(i) for i in tokenized_data], bins=50)\n","plt.xlabel('length')\n","plt.ylabel('count')\n","plt.show()"],"metadata":{"id":"5y5pXl7h5mn5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model= Word2Vec(sentences=tokenized_data,vector_size=100,window=10,min_count=3,sg=0)"],"metadata":{"id":"Ia8rOfrO9uwe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model.wv.vectors.shape)\n","print(model.wv.most_similar('이승기')) #데이터의 양이 많아지니까 유사도를 인정할수 있는 결과가 나온다."],"metadata":{"id":"TIxZYSTgDTxf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y45ULqs7D1Z5"},"execution_count":null,"outputs":[]}]}
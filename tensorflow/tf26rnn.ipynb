{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyP9ya2ENYugo/vue2fCPjSI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["문자열 토큰 처리 후 LSTM으로 감성분류(?)\n","이항분류하려고 하는데 긍정인가 부정인가를 분류하려고 함\n"],"metadata":{"id":"kaPybrxtCWzD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"utK0ACmICUOe"},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","from keras.models import Sequential\n","import numpy as np\n","from keras.layers import SimpleRNN,LSTM,Dense\n"]},{"cell_type":"code","source":["samples=['The cat say on the mat.', 'The dog ate my homework']\n","\n","token_index = {}\n","for sam in samples:\n","    for word in sam.split(sep=' '):\n","        if word not in token_index:\n","            token_index[word]=len(token_index)\n","print(token_index)\n","print()\n","tokenizer = Tokenizer(num_words=10)\n","tokenizer.fit_on_texts(samples)\n","token_seq = tokenizer.texts_to_sequences(samples)\n","\n","print(token_seq)\n","print(tokenizer.word_index)\n","print()\n","token_mat=tokenizer.texts_to_matrix(samples, mode='binary') #'count','tdidf','feq'\n","print(token_mat) #원핫 이진벡터 형태\n","print(tokenizer.word_counts)\n","print(tokenizer.document_count)\n","print(tokenizer.word_docs)\n","print()\n","\n","from keras.utils import to_categorical\n","token_seq = to_categorical(token_seq[0], num_classes=6)\n","print(token_seq)"],"metadata":{"id":"1_U7kXPgV8rI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs = ['너무 재밌네요','최고에요','참 잘 만든 영화에요','추천하고 싶은 영화입니다','한 번 더 보고 싶어요',\n","        '글쎄요','별로에요','생각보다 지루하네요','연기가 어색해요','재미없어요']\n","labels=np.array([1,1,1,1,1,0,0,0,0,0])\n","\n","token=Tokenizer()\n","token.fit_on_texts(docs)\n","print(token.word_index)\n","\n","x= token.texts_to_sequences(docs)\n","print('정수 인덱싱된 토큰 결과: ',x)\n","\n","from keras.utils import pad_sequences #서로 다른 크기의 데이터를 일정한 크기로 만들어줌\n","from keras.layers import Embedding,Flatten\n","padded_x = pad_sequences(x, 5)\n","print(padded_x)\n","\n","word_size= len(token.word_index)+1 # 임베딩에 입력될 단어수(토큰 수)를 지정 : 가능한 토큰 수는 최대값 +1을 준다.\n","model = Sequential()\n","# Embedding(가능토큰수, 벡터크기, input_length=시퀀스개수) - 워드 임베딩이란 텍스트 크기 단어를 밀집 벡터로 만드는 작업\n","# Embeddding은 워드 임베딩작업을 수행하고 3D 텐서를 리턴함\n","model.add(Embedding(word_size,8,input_length=5))#시퀀스 수 RNN 층\n","model.add(LSTM(units=32, activation='tanh')) #RNN층\n","# model.add(Flatten()) #빼고 그냥 해도 된다. label이기 때문에\n","model.add(Dense(units=32, activation='relu')) #Dense layer\n","model.add(Dense(units=1, activation='sigmoid')) #완전연결층\n","print(model.summary())\n","\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","model.fit(padded_x,labels,epochs=20,verbose=1)\n","print('eval : %.4f'%(model.evaluate(padded_x, labels)[1]))\n","\n","print('predict : ', np.where(model.predict(padded_x)>0.5,1,0).ravel())\n","print('real : ', labels)\n","\n","#lstm을 덴스 층에 올려놓고 이항분류를 해본것 참 거짓을 파악한 것"],"metadata":{"id":"4ZKLhJ6sYRGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 시스템만 좋으면 됨 알고리즘만 좋으면 됨\n","# 위키 이런데 가면 데이터가 많다.\n","\n"],"metadata":{"id":"mjcKCIDFa2G8"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN45RttzgyxWxCmVyYx0ZmP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# IMdb dataset\n","으로 감성분류(이항분류) - LSTM+Dens, Conv+Dense"],"metadata":{"id":"uEfKWPRfPbpM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8mI3_ROPaLV"},"outputs":[],"source":["from keras.datasets import imdb\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Embedding, Dropout\n","\n","(x_train, y_train),(x_test,y_test) = imdb.load_data(num_words=10000) # 숫자 제한 10000\n","print(x_train.shape, y_train.shape,x_test.shape,y_test.shape) # (25000,) (25000,) (25000,) (25000,)\n","print(x_train[:1])\n","print(y_train[:1], set(y_train)) #{0, 1}"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","len_data = [len(i) for i in x_train]\n","print('요소 최대 크기 : ', np.max(len_data)) # 2494\n","print('요소 크기 평균 : ', np.mean(len_data)) # 238.71364\n","# plt.boxplot(len_data)\n","# plt.show()\n","\n","#x_train에 등록된 인덱스에 해당하는 단어 출력\n","word_to_index=imdb.get_word_index()\n","index_to_word = {}\n","for k, v in word_to_index.items():\n","    # print(k)\n","    # print(v)\n","    index_to_word[v + 3]=k\n","\n","print(index_to_word) #{34701: 'fawn', 52006: 'tsukino', 52007: 'nunnery',\n","print('빈도수 1등 : ', index_to_word[4])\n","print('빈도수 100등 : ', index_to_word[103])\n","print()\n","# imdb는 pad 부분은 0, 문장 시작은 1, unknown은 2로 채워져 있다.\n","for idx, token in enumerate(('<pad>','<sos>','<unk>')): #애네 회사가 이렇게 만들었기에 우리도 이렇게 쓸수 밖에 없음\n","    index_to_word[idx]=token\n","\n","print(' '.join([index_to_word[i] for i in x_train[0]]))"],"metadata":{"id":"IOrXF8KzRhue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.utils import pad_sequences\n","# early stopping주기\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","max_len = 500  #리뷰 최대길이는 500으로 제한\n","\n","x_train = pad_sequences(x_train, maxlen=max_len)\n","x_test = pad_sequences(x_test, maxlen=max_len)\n","# print(x_train[:1])\n","\n","# 모델 작성 방법1 : LSTM + Dense\n","model = Sequential()\n","model.add(Embedding(10001,200,input_length=max_len))\n","model.add(LSTM(128,activation='tanh'))\n","model.add(Dense(1, activation='sigmoid'))\n","print(model.summary()) #Total params: 2168777 (8.27 MB)\n","\n","model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['acc'])\n","\n","es = EarlyStopping(monitor ='val_loss', mode='auto', patience=5, baseline=0.01)\n","#baseline :특정값을 정해놓고 이 값에 도달하면 patience를 종료하는 것\n","mc= ModelCheckpoint('tf37m1.hdf5',monitor='val_loss',save_best_only=True)\n","\n","\n","history = model.fit(x_train,y_train, validation_split=0.2, batch_size=64, epochs=100, callbacks=[es, mc], verbose=2)\n","\n","print('acc: ',history.history['acc'])\n","print('loss: ',history.history['loss'])\n","print('evaluate: ', model.evaluate(x_test,y_test)) #evaluate:  [0.7049450874328613, 0.8182799816131592]\n","\n"],"metadata":{"id":"PhATyCTmSOeD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pred\n","from keras.models import load_model\n","mymodel = load_model('tf37m1.hdf5')\n","pred=mymodel.predict(x_test)\n","print('예측값 : ', pred[:10].flatten())\n","print('실제값 : ',y_test[:10])\n"],"metadata":{"id":"2PK8Krccd3Ay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 작성 방법2 : Conv1D + Dense\n","from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Dropout\n","\n","model = Sequential()\n","model.add(Embedding(10001,200,input_length=max_len))\n","model.add(Conv1D(filters=128,kernel_size=3, padding='valid',strides=1,activation='relu'))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dropout(0.3))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","print(model.summary()) #Total params: 2085449 (7.96 MB)\n","\n","model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['acc'])\n","\n","es = EarlyStopping(monitor ='val_loss', mode='auto', patience=5, baseline=0.01)\n","#baseline :특정값을 정해놓고 이 값에 도달하면 patience를 종료하는 것\n","mc= ModelCheckpoint('tf37m2.hdf5',monitor='val_loss',save_best_only=True)\n","\n","\n","history = model.fit(x_train,y_train, validation_split=0.2, batch_size=64, epochs=100, callbacks=[es, mc], verbose=2)\n","\n","print('acc: ',history.history['acc'])\n","print('loss: ',history.history['loss'])\n","print('evaluate: ', model.evaluate(x_test,y_test)) #evaluate:  [0.7049450874328613, 0.8182799816131592]"],"metadata":{"id":"r_Cc-g_NeYhr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 시각화\n","vloss= history.history['val_loss']\n","loss=history.history['loss']\n","epoch = np.arange(len(loss))\n","plt.plot(epoch, vloss, marker='.',c='red',label='val_loss')\n","plt.plot(epoch, loss, marker='s',c='blue',label='loss')\n","plt.legend()\n","\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()\n","\n","#전통적인 방법 LSTM빼고 Dense만 써주어야 한다."],"metadata":{"id":"X4CsdPvPf9kh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pred\n","mymodel = load_model('tf37m2.hdf5')\n","pred=mymodel.predict(x_test)\n","print('예측값 : ', pred[:10].flatten())\n","print('실제값 : ',y_test[:10])"],"metadata":{"id":"Phi9HJsZgqcg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#새로운 영화평 값으로 감성분류 tensorflow -p21\n","# IMDB dataset 긍부정 예측 함수\n","import re\n","def sentiment_predict(new_sentence):\n","  new_sentence = re.sub('[^0-9a-zA-Z ]', '', new_sentence).lower()\n","  # 정수 인코딩\n","  encoded = []\n","  for word in new_sentence.split():\n","    # 단어 집합의 크기를 10,000으로 제한.\n","    try :\n","      if word_to_index[word] <= 10000:\n","         encoded.append(word_to_index[word]+3)\n","      else:\n","         encoded.append(2)   # 10,000 이상의 숫자는 <unk> 토큰으로 취급.\n","    except KeyError:\n","      encoded.append(2)     # 단어 집합에 없는 단어는 <unk> 토큰으로 취급.\n","\n","  pad_new = pad_sequences([encoded], maxlen = max_len)  # 패딩\n","\n","  # 예측하기\n","  score = float(mymodel.predict(pad_new))\n","  if(score > 0.5):\n","     print(\"{:.2f}% 확률로 긍정!.\".format(score * 100))\n","  else:\n","     print(\"{:.2f}% 확률로 부정!\".format((1 - score) * 100))\n","\n","# 긍/부정 분류 예측\n","temp_str = \"This movie was just way too overrated. The fighting was not professional.\"\n","sentiment_predict(temp_str)\n","\n","temp_str = \"good\"\n","sentiment_predict(temp_str)\n","\n","temp_str = \"I was lucky enough to be included in the group to see the advanced screening in Seoul. And,  I need to say a big thank-you to Marvel Studios.\"\n","sentiment_predict(temp_str)\n","\n","temp_str = \"bad\"\n","sentiment_predict(temp_str)\n","\n","# 이미지 뿐만 아니라 자연어도 잘 처리한다.\n","# rnn이 이것뿐만 아니라 서로 잘 처리한다.\n","# dense에 넘기기전에 cnn 이냐 rnn인지 판단해야 하는데 이것은 데이터에 따라 달라진다.\n"],"metadata":{"id":"XCDqn1eHhM7N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XdCx5GVRiqOv"},"execution_count":null,"outputs":[]}]}
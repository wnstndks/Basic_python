{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPezGSZ2g9OWEvDRxLL1eul"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["countvectorizer: 출현 빈돋만 생성하기에 약점을 가지고 있음\n","\n","tidf를 이용해 많이 나오기는 하지만,\n","하나의 문장내에서는 단어들에 가중치를 주어 tfidfvectorizer를 준다."],"metadata":{"id":"236amRQ4SSN9"}},{"cell_type":"markdown","source":["# 자연어 처리에서 특징추출\n","###독립변수 나올때 특징 추출이 아님(다른 얘기)\n","\n","여기에서의 특징추출이란 단어나 문장들을 어떤 특징 값으로 변환하는 것을 의미한다.\n","문자로 구성된 데이터를 모델에 적용할 수 있도록, 특징을 추출해 수치화한다.\n","단어의 수를 파악해 문장을 분석하는 방법"],"metadata":{"id":"Gb9xXiSaS053"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wfx1oEGRPDdG"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","textData = ['나는 배 고프다 아니 배가 고프다.','나는 내일 점심 뭐 먹지?','나는 내일 공부 해야겠다.','나는 점심 먹고 공부 해야지!']\n","count_vec = CountVectorizer(analyzer='word',ngram_range=(1,1), stop_words=['나는']) # ngram_range : 단어장 생성에 사용할 토큰 크기를 결정함\n","count_vec.fit(textData)\n","print(count_vec.get_feature_names_out()) # 전치사를 빼고 싶을 때는 konlpy를 걸어주면 된다.\n","# 문장에서 공백을 기준으로 단어별로 잘라주고 있음\n","# 한글자짜리는 제외하고 구두점은 빼주고 있다 -> 단어 사전을 만들고 있음 => vocabulary를 확인해주면 된다.\n","print(count_vec.vocabulary_)\n","# {'고프다': 0, '아니': 6, '배가': 5, '내일': 2, '점심': 7, '먹지': 4, '공부': 1, '해야겠다': 8, '먹고': 3, '해야지': 9} -> 가나다 순으로 인덱싱을 함\n","# dict에 요소는 10개임\n","print([textData[0]])\n","\n","sentence=[textData[0]]\n","print(count_vec.transform(sentence)) #벡터화\n","print(count_vec.transform(sentence).toarray())\n","# 각 sentence가 있으면 vocabulary를 만들어주고 토큰 별로 잘라줌"]},{"cell_type":"markdown","source":["# TF-IDF\n","Term Fequency(1개의 문서 내 특정 단어 등장빈도) - inverse Document Frequency\n","\n","\n","## DF- 특정 단어가 나타나는 문장 수\n","정보 검색과 텍스트 마이닝에서 이용하는 가중치로, 여러 문서로 이루어진 문서군이 있을 때 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치이다\n","\n","단순히 빈도수로 그 단어의 가치를 정하는 것이 아니라, 여러문장에 많이 등장하는 단어는 패널티를 주어 단어 빈도의 스케일을 맞추는 기법.\n","\n"],"metadata":{"id":"HmODw06kWZYE"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","textData = ['나는 배 고프다 아니 배가 고프다.','나는 내일 점심 뭐 먹지?','나는 내일 공부 해야겠다.','나는 점심 먹고 공부 해야지!']\n","tfidf_vec = TfidfVectorizer(analyzer='word',ngram_range=(1,1), stop_words=['나는'])\n","tfidf_vec.fit(textData)\n","print(tfidf_vec.get_feature_names_out())\n","print(tfidf_vec.vocabulary_)\n","print(tfidf_vec.transform(sentence).toarray())\n","print()\n","\n","sentence= [textData[3]]\n","print(sentence)\n","print(tfidf_vec.transform(sentence))\n","print(tfidf_vec.transform(sentence).toarray())\n","\n"],"metadata":{"id":"m8vU2V-OPXdE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TfidfVectorizer\n","를 사용해 텍스트(형태소 분석기 Okt를 적용)를 벡터로 변환 후 단어 간 유사도 계산"],"metadata":{"id":"_yw5iFaNdT_B"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"id":"3Zgc4XGKY_GS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from konlpy.tag import Okt\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","okt=Okt()\n","\n","def tokenizeFunc(ss): # 형태소(일정한 의미가 있는 작은 단위) 분석용 수행 함수 - Okt가 해줌\n","    ss=okt.normalize(ss) #Okt는 정규화를 일부 지원함. 예)'사랑햌' -> '사랑해'로 수정\n","    ss = okt.morphs(ss) # 형태소 단위로 분리, 반환형은 리스트\n","    return ss\n","\n","texts = ['길동이는 파이썬을 좋아합니다','길동이는 웹을 잘 합니다','길동이는 운동을 매우 잘합니다'] # 스포츠와 관련된 내용\n","new_texts = ['길동이는 파이썬을 좋아하고 운동을 잘합니다.'] #글은 어느 장르에 속하는 글이다.\n","# 시계열 데이터가 아님, 순서는 상관이 없음\n","\n","tfidf = TfidfVectorizer(tokenizer=tokenizeFunc, token_pattern=None).fit(texts)\n","print(tfidf.vocabulary_)\n","tfidf_matrix = tfidf.fit_transform(texts)\n","print(tfidf_matrix)\n","print(tfidf_matrix.toarray()) # 같은 빈도라도 tfidf값이 높게 나온것\n"],"metadata":{"id":"6-QgGe4oddjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for ntext in new_texts:\n","    tftrans = tfidf.transform([ntext]) #새로운 문장을 벡터로 변환함 ['길동이는 파이썬을 좋아하고 운동을 잘합니다.']\n","    print('tftrans : \\n',tftrans)\n","    # 새로운 문장과 기존 문장들 사이의 코사인 유사도(데이터 크기차이에 관계가 없이 계산 가능) 계산\n","    cosine_simil = cosine_similarity(tftrans,tfidf_matrix)\n","    print('cosine_simil : ', cosine_simil) # [[0.6294     0.65051252 0.77272178]]\n","\n","    #출력\n","    print(f'새로운 문장 : {ntext}')\n","    print('----------'*10)\n","    print(f'기존 문장 : ')\n","    for idx in range(3):\n","        print(cosine_simil.argsort()[0]) # [0]은 2차원 배열의 0행\n","        print((idx+1)*-1)\n","        print(cosine_simil[0][(idx+1)*-1])\n","        most_simil_idx=cosine_simil.argsort()[0][(idx+1)*-1]\n","        print(most_simil_idx)\n","        most_simil_sentence = texts[most_simil_idx]\n","        simil_score = cosine_simil[0][most_simil_idx]\n","        print(f'{most_simil_sentence}(유사도:{simil_score:.3f})')\n","    print()\n","# 새로운 문장과 기존 문장과의 유사도가 있다 - 유사도가 높은 것을 토대로 판단한다.\n","# Dense란? 자연어 처리는 순서가 있다 순서가 있는 자연어를 가지고 문장 생성 또는 번역 또는 챗봇 요약 문서에 이미지에 설명 달기 주식 에측 이런 것들에 소리로 번역, 예약 시스템 안내 시스템 등을 할 수 있다. 자연스럽게 바뀔수 있음\n"],"metadata":{"id":"Sqndpi_shGMS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"u2LZsfyjimWv"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyO/vD03hGY4jIITPnby7a1x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# daum 페이지의 뉴스 기사를 읽어 형태소 분석 후 word2vec을 적용\n","####<단어 간 유사도 확인하기>\n"],"metadata":{"id":"MgaULeiRR57D"}},{"cell_type":"code","source":["!pip install konlpy  #konlpy는 자바로 만들어졌으므로 자바가 설치되어있어야 함"],"metadata":{"id":"79umFCDDSU_f"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pl2h1MeiR2qA"},"outputs":[],"source":["from konlpy.tag import Okt\n","import pandas as pd\n","\n","okt= Okt()\n","\n","with open('news.txt',mode='r') as obj:\n","    lines=obj.read().split('\\n')\n","\n","print(lines)"]},{"cell_type":"code","source":["# 명사만 추출해 단어 빈도 수를 확인\n","wordDic= {}\n","\n","for line in lines:\n","    datas= okt.pos(line) # 품사태깅 - 단어마다 품사가 옆에 따라 붙는 것\n","    # print(datas)\n","    for word in datas:\n","        if word[1]=='Noun':\n","            # print(word[0])\n","            if len(word[0]) >=2:\n","                if not(word[0] in wordDic):\n","                    wordDic[word[0]] = 1\n","                wordDic[word[0]]+=1\n","\n","print(wordDic) # counting을 해서 이 기사에 대한 정보를 알수 있다.\n","\n","keys = sorted(wordDic.items(), key=lambda x:x[1], reverse=True)\n","print(keys)\n","print()\n","\n","# keys 자료를 DataFrame에 담기\n","wordList = []\n","countList= []\n","\n","for word, count in keys[:20]:\n","    wordList.append(word)\n","    countList.append(count)\n","\n","df=pd.DataFrame()\n","df['word'] = wordList\n","df['count'] = countList\n","\n","print(df.head(3))\n","\n","# pandas의 기능으로 기술통계 작업 처리를 해주면 됨..."],"metadata":{"id":"sEAR6GMLSR8v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 단어 간 유사도 구하기\n","results= [] #한글만 기억\n","\n","with open('news.txt',mode='r') as obj:\n","    lines=obj.read().split('\\n')\n","\n","    for line in lines:\n","        datas = okt.pos(line, stem=True) # stemming - 동사나 형용사에 대해서 원형 어근으로 출력, 한가한 -> 한가하다\n","        print(datas)\n","        imsi = []\n","        for word in datas:\n","            if not word[1] in ['Number','Josa','Punctuation','Alpha','Modifier','Suffix','Foregin']:\n","                if len(word[0]) >= 2:\n","                    imsi.append(word[0])\n","        imsi2 = (' '.join(imsi)).strip()\n","        results.append(imsi2)\n"],"metadata":{"id":"D6kMjbwCU-78"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(results)\n","\n","fileName ='daumnews.txt'\n","with open(fileName, mode='w') as obj:\n","    obj.write('\\n'.join(results))\n","\n","from gensim.models import word2vec\n","# 코사인 유사도를 이용해 단어와 단어사이 수치를 알려줌\n","\n","lineObj = word2vec.LineSentence(fileName)\n","\n","model = word2vec.Word2Vec(sentences=lineObj, vector_size=100, window=10, min_count=1, sg=1)# 2보다 작은 녀석은 제외\n","# sg=0: CBOW(주변단어로 중심단어를 예측), sg=1: Skip-Gram(중심단어로 주변단어를 예측)\n","# 주변 단어를 몇개를 창조하는 것은 윈도우로 결정함\n","# 100차원의 벡터를 만듦. 주변단어를 예측하는 알고리즘을 이용해서 모델이 학습을 함\n","print(model)\n","\n","# model.init_sims(replace=True) # 필요없는 메모리 해제\n","\n","# positive : 단어 사전에 해당 단어가 있을 확률, 가까운 단어를 찾음\n","# negative : 단어 사전에 해당 단어가 없을 확률, 먼 단어를 찾음\n","print(model.wv.most_similar(positive=['아메리카노'])) #수출이란 단어와 유사도에서 긍정적 기여 목록\n","print(model.wv.most_similar(negative=['시키다'])) #수출이란 단어와 유사도에서 부정 기여 목록\n","print(model.wv.most_similar(negative=['분비'], topn=5))  #기여도가 높은 녀석 다섯개만 나온다.\n"],"metadata":{"id":"HaMORCCwZDwQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["희소표현\n","\n","밀집벡터 - 공간을 내 마음대로 데이터들을 표현할 수 있는 것\n","데이터들을 0에서부터 1사이에 가둘수 있는 것\n","차원을 내가 정해주고 그 차원내에서 수치들이 표현될 수 있도록 하는 것\n"],"metadata":{"id":"6iOQx-ZGeI8o"}},{"cell_type":"code","source":[],"metadata":{"id":"ZeaOzFLZaK18"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMyUJsB5U3VZitFsm9tYm3l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8tZI3gBxVrh9"},"outputs":[],"source":["# 한글이니까 konlpy 설치\n","!pip install konlpy"]},{"cell_type":"code","source":["# 긍정은 1이고 부정은 0이다.\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import re\n","import urllib.request\n","from konlpy.tag import Okt\n","from keras.utils import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","import tensorflow as tf"],"metadata":{"id":"Aa83W38CV1Az"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# urllib.request.urlretrieve('https://raw.githubusercontent.com/pykwon/python/master/testdata_utf8/ratings_train.txt', filename='ratings_train.txt')\n","train_data=pd.read_table('https://raw.githubusercontent.com/pykwon/python/master/testdata_utf8/ratings_train.txt')\n","test_data=pd.read_table('https://raw.githubusercontent.com/pykwon/python/master/testdata_utf8/ratings_test.txt')\n","print(train_data[:2],train_data.shape) # (150000, 3)\n","print(test_data[:2], test_data.shape) # (50000, 3)\n","\n","# data cleaning\n","print(train_data['document'].nunique(), test_data['document'].nunique()) #중복자료 있다.\n","\n","train_data.drop_duplicates(subset=['document'], inplace=True)\n","print('샘플 수 : ', len(train_data))\n","print(train_data.groupby('label').size().reset_index(name='count')) #레이블별 건수 출력\n","\n","print(train_data.isnull().values.any())\n","train_data=train_data.dropna(how='any')\n","print(train_data.shape)\n","\n","# 한글 자료만 허용\n","train_data['document'] = train_data['document'].str.replace('[^가-힣 ]','')\n","print(train_data[:3])\n","\n","# white space를 empty value로 변경(문단의 시작글자가 공백인것)\n","train_data['document'] = train_data['document'].str.replace('^ +', '') #문단의 시작글자가 공백일 경우 지우라는 뜻\n","train_data['document'].replace('',np.nan, inplace=True)\n","print(train_data.isnull().sum())\n","print(train_data.loc[train_data.document.isnull()][:3])\n","\n","train_data=train_data.dropna(how='any')\n","print(train_data.shape) #(143028, 3)\n","\n","test_data.drop_duplicates(subset=['document'],inplace=True)\n","test_data['document'] = test_data['document'].str.replace('[^가-힣 ]','')\n","test_data['document'] = test_data['document'].str.replace('^ +', '') #문단의 시작글자가 공백일 경우 지우라는 뜻\n","test_data['document'].replace('',np.nan, inplace=True)\n","test_data=test_data.dropna(how='any')\n","print(test_data.shape) #(143028, 3)"],"metadata":{"id":"9TlkqLuUZpKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 불용어\n","stopwords = ['의','를','을','가','들','좀','잘','하여','으로','에','여','한','와','과','리','더','에는','하다','아','흠','별','막']\n","\n","from tqdm import tqdm\n","\n","okt=Okt()\n","x_train=[]\n","for sentence in tqdm(train_data['document']):\n","    tokenized_sentence = okt.morphs(sentence,stem=True)\n","    stop_remove_sentence = [word for word in tokenized_sentence if not word in stopwords] #불용어 제거\n","    x_train.append(stop_remove_sentence)\n","\n","print(x_train[:3])\n","\n","\n","x_test=[]\n","for sentence in tqdm(train_data['document']):\n","    tokenized_sentence = okt.morphs(sentence,stem=True)\n","    stop_remove_sentence = [word for word in tokenized_sentence if not word in stopwords] #불용어 제거\n","    x_test.append(stop_remove_sentence)\n","\n","print(x_test[:3])\n","\n","tokenizer=Tokenizer()\n","tokenizer.fit_on_texts(x_train)\n","print(tokenizer.word_index)\n","\n","\n","\n"],"metadata":{"id":"IezXHedibHbF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 등장빈도가 3회 보다 적은 단어들 비중 확인\n","threshold = 3\n","total_cnt = len(tokenizer.word_index)\n","rare_cnt = 0  # 등장빈도가 3회 보다 적은 단어들 건수\n","total_freq = 0  # 전체 단어 빈도수 - 훈련 데이터에 대한\n","rare_freq = 0  # 등장빈도가 3회 보다 적은 단어들 비율\n","\n","# 단어와 빈도수 k:v로 처리\n","for k, v in tokenizer.word_counts.items():\n","  total_freq = total_freq + v\n","  if v < threshold:\n","    rare_cnt = rare_cnt + 1\n","    rare_freq = rare_freq + v\n","\n","print('total_cnt : ', total_cnt)\n","print('rare_cnt : ', rare_cnt)\n","print('3회 미만 단어 비율 : ', (rare_cnt / total_cnt) * 100)\n","print('전체 빈도에서 3회 미만 빈도 비율 : ', (rare_freq / total_freq) * 100)\n","\n","# 전체 단어 수 중에서 빈도 수 2이하인 단어는 제거\n","vocab_size = total_cnt - rare_cnt + 1\n","print('vocab_size : ', vocab_size)"],"metadata":{"id":"3h6LHkONnd2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vocab_size : 19219\n","tokenizer = Tokenizer(vocab_size)\n","tokenizer.fit_on_texts(x_train)\n","x_train = tokenizer.texts_to_sequences(x_train)\n","x_test = tokenizer.texts_to_sequences(x_test)\n","print(x_train[:3])\n","\n","y_train = np.array(train_data['label'])\n","y_test = np.array(test_data['label'])\n","\n","# empty smaple 제거 : 빈도수가 낮은 단어가 삭제됨으로 해서 빈도수가 낮은 단어로 구성된 샘플들은 empty sample이 됨\n","# 그러므로 제거작업이 필요\n","drop_train = [index for index, sentence in enumerate(x_train) if len(sentence) < 1]\n","print(drop_train)\n","\n","x_train = np.delete(x_train, drop_train, axis=0)\n","y_train = np.delete(y_train, drop_train, axis=0)\n","print(len(x_train), len(y_train))\n","\n","# padding\n","print('리뷰 최대 길이 : ', max(len(review) for review in x_train))\n","print('리뷰 평균 길이 : ', sum(map(len, x_train)) / len(x_train))\n","\n","max_len = 30  # padding의 요소 수는 대략 30을 부여\n","x_train = pad_sequences(x_train, maxlen=max_len)\n","x_test = pad_sequences(x_test, maxlen=max_len)"],"metadata":{"id":"RJv30teara7r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model\n","from keras.layers import LSTM, Embedding, Dense\n","from keras.models import Sequential\n","from keras.models import load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, 100))  # 차원수는 100차원 줌\n","model.add(LSTM(128, activation='tanh'))\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","\n","es = EarlyStopping(monitor='val_loss', mode='auto', patience=5)\n","mc = ModelCheckpoint('tf38.hdf5', monitor='val_loss', save_best_only=True)\n","\n","history = model.fit(x_train, y_train, validation_split=0.2, batch_size=64, callbacks=[es, mc], verbose=2)\n","\n","print('test 정확도 : ', model.evaluate(x_test, y_test)[1])\n","print('test 손실도 : ', model.evaluate(x_test, y_test)[0])"],"metadata":{"id":"mE1vYvTau0Mq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mymodel = load_model('tf38.hdf5')\n","\n","# 리뷰 예측\n","def predictFunc(new_sen):\n","  new_sen = re.sub(r'[^가-힣 ]', '', new_sen)  # 가~힣, 공백을 제거한 나머지만 남김\n","  new_sen = okt.morphs(new_sen, stem=True)  # 어간처리해줌\n","  new_sen = [word for word in new_sen if not word in stopwords]\n","  encoded = tokenizer.texts_to_sequences([new_sen])\n","  pad_new = pad_sequences(encoded, maxlen=max_len)\n","  score = float(mymodel.predict(pad_new))\n","  if score > 0.5:\n","    print('{:.2f}% 확률로 긍정'.format(score * 100))\n","  else:\n","    print('{:.2f}% 확률로 부정'.format(score * 100))\n","\n","predictFunc('원작의 긴장감을 제대로 살려내지 못했다')\n","predictFunc('액션이 없는데도 재미 있는 몇안되는 영화')\n","predictFunc('보면서 웃지 않으면 환불')\n","predictFunc('엄청나게 재미없는 영화')\n","predictFunc('이 영화 맛도리다')"],"metadata":{"id":"tBi3DYluu0lb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QPHH5-L4u03e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PIShs1b2u1Cb"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMBbK8tcIikFcFjV6xfcJLh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# token이 자소 단위인 데이터로 자연어 생성 모델 작성"],"metadata":{"id":"CmeG76b25snT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZnKPdIl5R7Z"},"outputs":[],"source":["!pip install jamotools"]},{"cell_type":"code","source":["# token이 자소 단위인 데이터로 자연어 생성 모델 작성\n","import jamotools\n","import tensorflow as tf\n","import numpy as np\n","import keras\n","\n","path = keras.utils.get_file(\n","    'rnn_short_toji.txt',\n","    origin='https://raw.githubusercontent.com/pykwon/etc/master/rnn_test_toji.txt')\n","text = open(path, 'rb').read().decode(encoding='utf-8')\n","\n","print('전체 글자 수 : {}'.format(len(text)))\n","print(text[:100])"],"metadata":{"id":"P21iTNVb5r-t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 한글 자료 자모 단위로 분리. 한자에는 영향이 없다.\n","# s_split = jamotools.split_syllables(text[:50])\n","# print(s_split)\n","# rever = jamotools.join_jamos(s_split)\n","# print(rever)\n","# print(text[:50] == rever)\n","\n","train_text_x = jamotools.split_syllables(text)\n","# 단어 사전\n","vocab = sorted(set(train_text_x))\n","vocab.append('UNK')\n","print('{} unique words'.format(len(vocab)))   # 179 unique words\n","\n","char2idx = {w:i for i, w in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","print(idx2char)     # ['\\n' ',' ',,?' ... '힘찼으며' '힝' 'UNK']\n","text_as_int = np.array([char2idx[c] for c in train_text_x])\n","print(text_as_int)  # [69 81  2 ...  2  1  0]\n","print(len(text_as_int))   # 1345233\n","print(train_text_x[:20])  # ㅈㅔ 1 ㅍㅕㄴ ㅇㅓㄷㅜㅁㅇㅢ ㅂㅏㄹ\n","print(text_as_int[:20])   # [69 81  2 13  2 74 82 49  2 68 80 52 89 62 68 95  2 63 76 54]"],"metadata":{"id":"-kk30ASs6QTr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset 작성\n","seq_length = 80   # 80개의 자모가 주어질 경우 다음 자모를 예측\n","example_per_epoch = len(text_as_int) // seq_length\n","print(example_per_epoch)   # 16815\n","\n","import tensorflow as tf\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)   # 전체가 아니라 부분 데이터 읽기\n","char_dataset = char_dataset.batch(seq_length + 1, drop_remainder=True)\n","\n","for item in char_dataset.take(1):   # batch를 한번씩 불러옴\n","  print(item.numpy())\n","  print(idx2char[item.numpy()])\n","\n","print()\n","def split_input_target(chunk):\n","  return [chunk[:-1], chunk[-1]]   # [25단어], 1단어\n","\n","train_dataset = char_dataset.map(split_input_target)\n","\n","for x, y in train_dataset.take(1):\n","  print(idx2char[x.numpy()])\n","  print(x.numpy())\n","  print(idx2char[y.numpy()])\n","  print(y.numpy())"],"metadata":{"id":"3cci5fkA68dg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model\n","BATCH_SIZE = 64\n","steps_per_epoch = example_per_epoch // BATCH_SIZE\n","BUFFER_SIZE = 5000\n","\n","# shuffle을 사용하면 epoch 마다 Dataset을 섞을 수 있다. 과적합 방지에 효과적\n","train_dataset = train_dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n","\n","total_chars = len(vocab)\n","print(total_chars)   # 54048\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(total_chars, 100, input_length=seq_length),\n","    tf.keras.layers.LSTM(units=256),\n","    tf.keras.layers.Dropout(rate=0.2),\n","    tf.keras.layers.Dense(units=256, activation='relu'),\n","    tf.keras.layers.Dropout(rate=0.2),\n","    tf.keras.layers.Dense(units=total_chars, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","print(model.summary())"],"metadata":{"id":"uDmCGeis9VJN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 단어 단위 생성 모델 학습\n","from keras.preprocessing.sequence import pad_sequences\n","\n","def testmodelFunc(epoch, logs):\n","  if epoch % 5 != 0 and epoch != 49:  # 5의 배수이거나 49이면 처리\n","    return\n","\n","  test_sentence = text[:48]\n","  test_sentence = jamotools.split_syllables(test_sentence)\n","\n","  next_chars = 300\n","  for _ in range(next_chars):\n","    test_text_x = test_sentence[-seq_length:]  # 임의의 문장 뒤에서 부터 seq_length(25) 만큼 선택\n","    test_text_x = np.array([char2idx[c] if c in char2idx else char2idx['UNK'] for c in test_text_x])\n","    test_text_x = pad_sequences([test_text_x], maxlen=seq_length, padding='pre', value=char2idx['UNK'])\n","    output_idx = np.argmax(model.predict(test_text_x)[0])  # 출력값 중에서 가장 값이 큰 인덱스 반환\n","    test_sentence += idx2char[output_idx]  # 출력 단어는 test_sentence에 누적해 다음 작업의 입력으로 활용\n","  print()\n","  print(jamotools.join_jamos(test_sentence))  # 자모 단위로 결합\n","  print()"],"metadata":{"id":"TRv_0hi0-ybh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# epoch이 끝날 때 마다 testmodelFuncfmf 해 진행 결과를 출력.\n","# fit 할 때(학습 도중) 학습 데이터가 predict 되는 과정을 확인해 가며 작업하고 싶을 때 사용!\n","testModelCb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodelFunc)\n","\n","# repeat() : input을 반복. 1개의 에폭의 끝과 다음 에폭의 시작에 상관없이 인자 만큼 반복함\n","history = model.fit(train_dataset.repeat(), epochs=50,\n","          steps_per_epoch = steps_per_epoch, # 한 에폭에 사용할 step 수를 지정. ex) 총45개 sample이 있고 배치사이즈가 3이라면 15스텝으로 지정\n","          callbacks=[testModelCb], verbose=2 )\n","\n","print(history.history['loss'][-1])\n","print(history.history['accuracy'][-1])\n","\n","model.save('tf32model.hdf5')"],"metadata":{"id":"lOxDIUYPAbam"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import load_model\n","model = load_model('tf31model.hdf5')\n","\n","# 임의 문장을 사용해 생성된 새로운 문장 확인\n","test_sentence = \"이날은 수수개비를 꺾어도 아이들은 매를 맞지 않는다\"\n","test_sentence = jamotools.split_syllables(test_sentence)\n","\n","next_chars = 500\n","for _ in range(next_chars):\n","    test_text_x = test_sentence[-seq_length:]  # 임의의 문장 뒤에서 부터 seq_length(25) 만큼 선택\n","    test_text_x = np.array([char2idx[c] if c in char2idx else char2idx['UNK'] for c in test_text_x])\n","    test_text_x = pad_sequences([test_text_x], maxlen=seq_length, padding='pre', value=char2idx['UNK'])\n","    output_idx = np.argmax(model.predict(test_text_x)[0])  # 출력값 중에서 가장 값이 큰 인덱스 반환\n","    test_sentence += idx2char[output_idx]  # 출력 단어는 test_sentence에 누적해 다음 작업의 입력으로 활용\n","\n","print(jamotools.join_jamos(test_sentence))  # 자모 단위로 결합"],"metadata":{"id":"aJ4AjHZzFZOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r_cTxO9tF8ni"},"execution_count":null,"outputs":[]}]}
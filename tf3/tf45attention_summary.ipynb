{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPVgEsTFLitCIL3p7aRforE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 어텐션을 이용한 텍스트 요약(Text Summarization with Attention mechanism)\n","\n","아마존 리뷰 데이터 사용"],"metadata":{"id":"45avReag4I6M"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y49A18ia39gL"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import matplotlib.pyplot as plt\n","from nltk.corpus import stopwords\n","from bs4 import BeautifulSoup\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import urllib.request\n","np.random.seed(seed=0)\n"]},{"cell_type":"code","source":["# Reviews.csv 파일을 data라는 이름의 데이터프레임에 저장. 단, 10만개의 행(rows)으로 제한.\n","data = pd.read_csv(r\"/content/Reviews_short.csv\", nrows = 100000)\n","print('전체 리뷰 개수 :',(len(data)))\n"],"metadata":{"id":"NAcBDZR75PQs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = data[['Text','Summary']]\n","print(data.head())\n","print(data.sample(10))\n"],"metadata":{"id":"7SIJ29-06WyA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n","print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())\n","\n","# text 열에서 중복인 내용이 있다면 중복 제거\n","data.drop_duplicates(subset=['Text'], inplace=True)\n","print(\"전체 샘플수 :\", len(data))\n","\n","print(data.isnull().sum())"],"metadata":{"id":"BAW5cqOi6kkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Null 값을 가진 샘플 제거\n","data.dropna(axis=0, inplace=True)\n","print('전체 샘플수 :',(len(data)))\n","\n","# 전처리 함수 내 사용\n","contractions = {\"'cause\": 'because',\n"," \"I'd\": 'I would',\n"," \"I'd've\": 'I would have',\n"," \"I'll\": 'I will',\n"," \"I'll've\": 'I will have',\n"," \"I'm\": 'I am',\n"," \"I've\": 'I have',\n"," \"ain't\": 'is not',\n"," \"aren't\": 'are not',\n"," \"can't\": 'cannot',\n"," \"could've\": 'could have',\n"," \"couldn't\": 'could not',\n"," \"didn't\": 'did not',\n"," \"doesn't\": 'does not',\n"," \"don't\": 'do not',\n"," \"hadn't\": 'had not',\n"," \"hasn't\": 'has not',\n"," \"haven't\": 'have not',\n"," \"he'd\": 'he would',\n"," \"he'll\": 'he will',\n"," \"he's\": 'he is',\n"," \"here's\": 'here is',\n"," \"how'd\": 'how did',\n"," \"how'd'y\": 'how do you',\n"," \"how'll\": 'how will',\n"," \"how's\": 'how is',\n"," \"i'd\": 'i would',\n"," \"i'd've\": 'i would have',\n"," \"i'll\": 'i will',\n"," \"i'll've\": 'i will have',\n"," \"i'm\": 'i am',\n"," \"i've\": 'i have',\n"," \"isn't\": 'is not',\n"," \"it'd\": 'it would',\n"," \"it'd've\": 'it would have',\n"," \"it'll\": 'it will',\n"," \"it'll've\": 'it will have',\n"," \"it's\": 'it is',\n"," \"let's\": 'let us',\n"," \"ma'am\": 'madam',\n"," \"mayn't\": 'may not',\n"," \"might've\": 'might have',\n"," \"mightn't\": 'might not',\n"," \"mightn't've\": 'might not have',\n"," \"must've\": 'must have',\n"," \"mustn't\": 'must not',\n"," \"mustn't've\": 'must not have',\n"," \"needn't\": 'need not',\n"," \"needn't've\": 'need not have',\n"," \"o'clock\": 'of the clock',\n"," \"oughtn't\": 'ought not',\n"," \"oughtn't've\": 'ought not have',\n"," \"sha'n't\": 'shall not',\n"," \"shan't\": 'shall not',\n"," \"shan't've\": 'shall not have',\n"," \"she'd\": 'she would',\n"," \"she'd've\": 'she would have',\n"," \"she'll\": 'she will',\n"," \"she'll've\": 'she will have',\n"," \"she's\": 'she is',\n"," \"should've\": 'should have',\n"," \"shouldn't\": 'should not',\n"," \"shouldn't've\": 'should not have',\n"," \"so's\": 'so as',\n"," \"so've\": 'so have',\n"," \"that'd\": 'that would',\n"," \"that'd've\": 'that would have',\n"," \"that's\": 'that is',\n"," \"there'd\": 'there would',\n"," \"there'd've\": 'there would have',\n"," \"there's\": 'there is',\n"," \"they'd\": 'they would',\n"," \"they'd've\": 'they would have',\n"," \"they'll\": 'they will',\n"," \"they'll've\": 'they will have',\n"," \"they're\": 'they are',\n"," \"they've\": 'they have',\n"," \"this's\": 'this is',\n"," \"to've\": 'to have',\n"," \"wasn't\": 'was not',\n"," \"we'd\": 'we would',\n"," \"we'd've\": 'we would have',\n"," \"we'll\": 'we will',\n"," \"we'll've\": 'we will have',\n"," \"we're\": 'we are',\n"," \"we've\": 'we have',\n"," \"weren't\": 'were not',\n"," \"what'll\": 'what will',\n"," \"what'll've\": 'what will have',\n"," \"what're\": 'what are',\n"," \"what's\": 'what is',\n"," \"what've\": 'what have',\n"," \"when's\": 'when is',\n"," \"when've\": 'when have',\n"," \"where'd\": 'where did',\n"," \"where's\": 'where is',\n"," \"where've\": 'where have',\n"," \"who'll\": 'who will',\n"," \"who'll've\": 'who will have',\n"," \"who's\": 'who is',\n"," \"who've\": 'who have',\n"," \"why's\": 'why is',\n"," \"why've\": 'why have',\n"," \"will've\": 'will have',\n"," \"won't\": 'will not',\n"," \"won't've\": 'will not have',\n"," \"would've\": 'would have',\n"," \"wouldn't\": 'would not',\n"," \"wouldn't've\": 'would not have',\n"," \"y'all\": 'you all',\n"," \"y'all'd\": 'you all would',\n"," \"y'all'd've\": 'you all would have',\n"," \"y'all're\": 'you all are',\n"," \"y'all've\": 'you all have',\n"," \"you'd\": 'you would',\n"," \"you'd've\": 'you would have',\n"," \"you'll\": 'you will',\n"," \"you'll've\": 'you will have',\n"," \"you're\": 'you are',\n"," \"you've\": 'you have'}\n"],"metadata":{"id":"YFaAbSO16p1d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NLTK의 불용어\n","import nltk\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","print('불용어 개수 :', len(stop_words))\n","print(stop_words)\n","\n","# 전처리 함수\n","def preprocess_sentence(sentence, remove_stopwords = True):\n","    sentence = sentence.lower() # 텍스트 소문자화\n","    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n","    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열  제거 Ex) my husband (and myself) for => my husband for\n","    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n","    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n","    sentence = re.sub(r\"'s\\b\",\"\",sentence) # 소유격 제거. Ex) roland's -> roland\n","    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n","    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n","\n","    # 불용어 제거 (Text)\n","    if remove_stopwords:\n","        tokens = ' '.join(word for word in sentence.split() if not word in stop_words if len(word) > 1)\n","    # 불용어 미제거 (Summary)\n","    else:\n","        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n","    return tokens\n","\n","temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n","temp_summary = 'Great way to start (or finish) the day!!!'\n","print(preprocess_sentence(temp_text))\n","print(preprocess_sentence(temp_summary, 0))\n","\n","# Text 열 전처리\n","clean_text = []\n","for s in data['Text']:\n","    clean_text.append(preprocess_sentence(s))\n","print(clean_text[:5])\n","\n","# Summary 열 전처리\n","clean_summary = []\n","for s in data['Summary']:\n","    clean_summary.append(preprocess_sentence(s, 0))\n","print(clean_summary[:5])\n","\n","# 길이가 공백인 샘플은 NULL 값으로 변환\n","data.replace('', np.nan, inplace=True)\n","print(data.isnull().sum())\n","\n","data.dropna(axis = 0, inplace = True)\n","print('전체 샘플수 :',(len(data)))\n","\n","# 길이 분포 출력\n","text_len = [len(s.split()) for s in data['Text']]\n","summary_len = [len(s.split()) for s in data['Summary']]\n","\n","print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n","print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n","print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n","print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n","print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n","print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n","\n","plt.subplot(1,2,1)\n","plt.boxplot(summary_len)\n","plt.title('Summary')\n","plt.subplot(1,2,2)\n","plt.boxplot(text_len)\n","plt.title('Text')\n","plt.tight_layout()\n","plt.show()\n","\n","plt.title('Summary')\n","plt.hist(summary_len, bins=40)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()\n","\n","plt.title('Text')\n","plt.hist(text_len, bins=40)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()\n"],"metadata":{"id":"eRURb8fp6_Qo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 패딩의 길이를 정하겠습니다\n","text_max_len = 50\n","summary_max_len = 8\n","\n","def below_threshold_len(max_len, nested_list):\n","  cnt = 0\n","  for s in nested_list:\n","    if(len(s.split()) <= max_len):\n","        cnt = cnt + 1\n","  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n","\n","below_threshold_len(text_max_len, data['Text'])\n","below_threshold_len(summary_max_len, data['Summary'])\n","\n","# 정해준 최대 길이보다 큰 샘플들은 제거하겠습니다.\n","data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n","data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n","print('전체 샘플수 :',(len(data)))\n","print(data.head())\n","\n","# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n","data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n","data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n","print(data.head())\n","\n","# 인코더의 입력, 디코더의 입력과 레이블을 각각 저장해줍니다.\n","encoder_input = np.array(data['Text'])\n","decoder_input = np.array(data['decoder_input'])\n","decoder_target = np.array(data['decoder_target'])\n","\n"],"metadata":{"id":"PV4MXBvf8BYa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터의 분리\n","indices = np.arange(encoder_input.shape[0])\n","np.random.shuffle(indices)\n","print(indices)\n","\n","encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]\n","\n","n_of_val = int(len(encoder_input)*0.2)\n","print('테스트 데이터의 수 :',n_of_val)\n","\n","encoder_input_train = encoder_input[:-n_of_val]\n","decoder_input_train = decoder_input[:-n_of_val]\n","decoder_target_train = decoder_target[:-n_of_val]\n","\n","encoder_input_test = encoder_input[-n_of_val:]\n","decoder_input_test = decoder_input[-n_of_val:]\n","decoder_target_test = decoder_target[-n_of_val:]\n","\n","print('훈련 데이터의 개수 :', len(encoder_input_train))\n","print('훈련 레이블의 개수 :',len(decoder_input_train))\n","print('테스트 데이터의 개수 :',len(encoder_input_test))\n","print('테스트 레이블의 개수 :',len(decoder_input_test))\n"],"metadata":{"id":"qpW-qPVS8iqT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 정수 인코딩\n","src_tokenizer = Tokenizer()\n","src_tokenizer.fit_on_texts(encoder_input_train)\n","\n","threshold = 7\n","total_cnt = len(src_tokenizer.word_index) # 단어의 수\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","for key, value in src_tokenizer.word_counts.items():\n","    total_freq = total_freq + value\n","\n","    # 단어의 등장 빈도수가 threshold보다 작으면\n","    if(value < threshold):\n","        rare_cnt = rare_cnt + 1\n","        rare_freq = rare_freq + value\n","\n","# 단어 집합의 크기를 8000으로 제한하겠습니다.\n","print('단어 집합(vocabulary)의 크기 :',total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n"],"metadata":{"id":"GbOrB_ch9orR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["src_vocab = 8000\n","src_tokenizer = Tokenizer(num_words = src_vocab)\n","src_tokenizer.fit_on_texts(encoder_input_train)\n","\n","# 텍스트 시퀀스를 정수 시퀀스로 변환\n","encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train)\n","encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n","\n","print(encoder_input_train[:3])"],"metadata":{"id":"ckOWtTYT96PM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 레이블에 해당하는 요약 데이터\n","tar_tokenizer = Tokenizer()\n","tar_tokenizer.fit_on_texts(decoder_input_train)\n","\n","# 등장 빈도수가 6회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해봅시다.\n","threshold = 6\n","total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n","rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n","rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","\n","# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","for key, value in tar_tokenizer.word_counts.items():\n","    total_freq = total_freq + value\n","\n","    # 단어의 등장 빈도수가 threshold보다 작으면\n","    if(value < threshold):\n","        rare_cnt = rare_cnt + 1\n","        rare_freq = rare_freq + value\n","\n","print('단어 집합(vocabulary)의 크기 :',total_cnt)\n","print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n","print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n","\n","#  정수 인코딩 과정에서 배제시키겠습니다.\n","tar_vocab = 2000\n","tar_tokenizer = Tokenizer(num_words = tar_vocab)\n","tar_tokenizer.fit_on_texts(decoder_input_train)\n","tar_tokenizer.fit_on_texts(decoder_target_train)\n","\n","# 텍스트 시퀀스를 정수 시퀀스로 변환\n","decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)\n","decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n","decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n","decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n","\n","print(decoder_input_train[:5])\n","print(decoder_target_train[:5])\n","\n"],"metadata":{"id":"Cwyh1vYa-Jux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  빈 샘플(empty samples) 제거\n","\n","drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n","drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n","\n","print('삭제할 훈련 데이터의 개수 :',len(drop_train))\n","print('삭제할 테스트 데이터의 개수 :',len(drop_test))\n","\n","encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n","decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n","decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n","\n","encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n","decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n","decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n","\n","print('훈련 데이터의 개수 :', len(encoder_input_train))\n","print('훈련 레이블의 개수 :',len(decoder_input_train))\n","print('테스트 데이터의 개수 :',len(encoder_input_test))\n","print('테스트 레이블의 개수 :',len(decoder_input_test))\n","\n","# 패딩하기\n","encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\n","encoder_input_test = pad_sequences(encoder_input_test, maxlen = text_max_len, padding='post')\n","decoder_input_train = pad_sequences(decoder_input_train, maxlen = summary_max_len, padding='post')\n","decoder_target_train = pad_sequences(decoder_target_train, maxlen = summary_max_len, padding='post')\n","decoder_input_test = pad_sequences(decoder_input_test, maxlen = summary_max_len, padding='post')\n","decoder_target_test = pad_sequences(decoder_target_test, maxlen = summary_max_len, padding='post')\n"],"metadata":{"id":"2dUrKeTD-YMZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  seq2seq + attention으로 요약 모델 설계 및 훈련시키기\n","from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","embedding_dim = 128\n","hidden_size = 256\n","\n","# 인코더, 인코더는 LSTM 층을 3개 쌓습니다.\n","encoder_inputs = Input(shape=(text_max_len,))\n","\n","# 인코더의 임베딩 층\n","enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n","\n","# 인코더의 LSTM 1\n","encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n","encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n","\n","# 인코더의 LSTM 2\n","encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n","encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n","\n","# 인코더의 LSTM 3\n","encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n","encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n","\n","# 디코더\n","decoder_inputs = Input(shape=(None,))\n","\n","# 디코더의 임베딩 층\n","dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n","dec_emb = dec_emb_layer(decoder_inputs)\n","\n","# 디코더의 LSTM\n","decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n","decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n","\n","# 디코더의 출력층\n","decoder_softmax_layer = Dense(tar_vocab, activation = 'softmax')\n","decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs)\n","\n","# 모델 정의\n","model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n","print(model.summary())\n"],"metadata":{"id":"QzMIcSdZ-lBZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 어텐션 함수를 직접 작성하지 않고 이미 저자의 깃허브에 작성된 어텐션을 사용할 것이므로 아래의 코드를 통해 attention.py 파일을 다운로드하고, AttentionLayer를 임포트합니다. (바다나우 어텐션입니다.)\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/20.%20Text%20Summarization%20with%20Attention/attention.py\", filename=\"attention.py\")\n","from attention import AttentionLayer\n","\n","# 어텐션 층(어텐션 함수)\n","attn_layer = AttentionLayer(name='attention_layer')\n","attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n","\n","# 어텐션의 결과와 디코더의 hidden state들을 연결\n","decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n","\n","# 디코더의 출력층\n","decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n","decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n","\n","# 모델 정의\n","model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n","print(model.summary())\n"],"metadata":{"id":"roghampW-y1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n","history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n","          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n","          batch_size = 256, callbacks=[es], epochs = 50)"],"metadata":{"id":"5_y3y3vB_MVh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","plt.show()\n","\n","\n","# 테스트를 위해 필요한 3개의 사전을 만듭니다.\n","\n","src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n","tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n","tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n","\n","# 인코더 설계\n","encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n","\n","# 테스트 단계의 디코더를 설계합니다.\n","# 이전 시점의 상태들을 저장하는 텐서\n","decoder_state_input_h = Input(shape=(hidden_size,))\n","decoder_state_input_c = Input(shape=(hidden_size,))\n","\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n","# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n","\n","# 어텐션 함수\n","decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n","attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n","decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n","\n","# 디코더의 출력층\n","decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat)\n","\n","# 최종 디코더 모델\n","decoder_model = Model(\n","    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n","    [decoder_outputs2] + [state_h2, state_c2])\n"],"metadata":{"id":"ePBgb9kv_Szg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트를 위해 사용되는 함수 decode_sequence를 설계합니다.\n","def decode_sequence(input_seq):\n","    # 입력으로부터 인코더의 상태를 얻음\n","    e_out, e_h, e_c = encoder_model.predict(input_seq)\n","\n","     # <SOS>에 해당하는 토큰 생성\n","    target_seq = np.zeros((1,1))\n","    target_seq[0, 0] = tar_word_to_index['sostoken']\n","\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n","\n","        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_token = tar_index_to_word[sampled_token_index]\n","\n","        if(sampled_token!='eostoken'):\n","            decoded_sentence += ' '+sampled_token\n","\n","        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n","        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n","            stop_condition = True\n","\n","        # 길이가 1인 타겟 시퀀스를 업데이트\n","        target_seq = np.zeros((1,1))\n","        target_seq[0, 0] = sampled_token_index\n","\n","        # 상태를 업데이트 합니다.\n","        e_h, e_c = h, c\n","\n","    return decoded_sentence\n","\n","# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq2text(input_seq):\n","    sentence=''\n","    for i in input_seq:\n","        if(i!=0):\n","            sentence = sentence + src_index_to_word[i]+' '\n","    return sentence\n","\n","# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq2summary(input_seq):\n","    sentence=''\n","    for i in input_seq:\n","        if((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n","            sentence = sentence + tar_index_to_word[i] + ' '\n","    return sentence\n"],"metadata":{"id":"VwJvMtu0_jJY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트 샘플 중 500번부터 1000번까지 테스트해봅시다.\n","for i in range(1, 10):\n","    print(\"원문 : \",seq2text(encoder_input_test[i]))\n","    print(\"실제 요약문 :\",seq2summary(decoder_input_test[i]))\n","    print(\"예측 요약문 :\",decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n","    print(\"\\n\")\n"],"metadata":{"id":"tgxHM7kW_tqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ya5oNu6D_zlb"},"execution_count":null,"outputs":[]}]}